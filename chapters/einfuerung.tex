\section{Background}

An audio engineer's typical job is to manage the balance of multiple tracks of audio signals. The dynamic range of a signal can be compressed, in order to give quiter passages more presence. Loud peaks can be limited to balance the overall loudness of a musical piece. Using equalizers an audio engineer can make enhance or supress specific frequencies of a track to make it more present in a mix. Effects like reverb, echo, or chorus can be used to give a track more space in a mix effecting the mood or ambience of a music piece. It is typcial that each track in a recording session will be processed by a chain of several specific audio processors.

20 years ago the equipment needed for this kind of processing filled large racks. Today all of these tasks run as plugins on the CPU.

In 1996 Steinberg GmbH, the developers of Cubase, a popular audio production application, released the VST interface specification and SDK.\cite{VST-wikipedia} The VST plugin standard was special because it allowed realtime processing of audio in the CPU and it allowed other developers to programm plugins which could be run from within Cubase. The VST plugin standard quickly had widespread industry acceptance and was adopted by most developers of audio production applications. Although alternative standards exist, VST is still the most widely adopted crossplatform standard.

The number of realtime plugins that could run on a CPU was limited by several factors, hard disc access speeds, bus speeds, amount of ram, OS schedulers for instance\cite{latency98}. User's didn't expect to be able to run more than 10 plugin instances at a time. Simply playing back multiple tracks of digital audio in realtime was so taxinig on the CPU that an application's graphical interface would quickly become unresponsive.

Today it's possible to playback hundreds of channels of audio and hundreds of plugins in realtime. But as the performance threshhold has risen, so to have the expectations. The algorithms driving today's plugins are much more complex that those from 1996. Plugins are available today that model physical systems or emulate the analog circutry of popular vintage synthsizers. So, even though CPU performance has increased significantly, it's still easy to reach the limits, especially with the more complex high qualtiy plugins.

Serval DSP based systems exist that can alleviate the load on the CPU much in the same way that GPU accelerator cards work. Audio processing jobs are delegated to external specialized hardware via PCIe or Thunderbolt interfaces. However, these DSP based accelerators are proprietary and expensive. Developing plugins for a DSP chip is also significatly more complex than developing for the CPU.

\section{Realtime Audio Plugins}

Music composition and production is typically done with the assistance of a music sequencing application. Midi events and audio recordings are arranged as tracks that can be mixed, edited, and processed. In order to make changes undo-able edits are made in a non-destructive fasion, calculated dynamically in realtime during audio playback. The original audio data is always preserved. The user can change the parameters of an effect or process in realtime and experiment with various parameters without fearing that the original audio recording might be permenantly altered.

A music sequencer or audio production application will usually include several built in realtime effects that a user can apply to an audio track. In addition to the built in options all professional applications will also be able to load 3d party effect plugins. Depending on the platform and vendor one or several available plugin standard will be implmeneted, the most common standard being Steinberg's VST standard.

Regardless of the standard most audio plugins function in a similar fashion. The host application will periodically poll the plugin via a callback, providing access to the source audio data stream and expecting the plugin to return the processed data.

Audio plugins can also provide a gui to the user that allows processing parameters to be modified, saved, and sequenced as well. This might be the cutoff frequency of a low pass filter, or the delay time of a reverb effect, for example.

On the Windows platform VST plugins are compiled to Dynamic Link Libraries, on Mac OSX they are Mach-O Bundles. The native apple Audio Unit plugins are also compiled as Mach-O bundles, they have almost identical functionality, but differ in their API implementation. Other alternative plugin formats are Avid's RTAS and AAX plugin formats, Microsoft's DirectX architecture, or LADSPA, DSSI and LV2 on Linux. From a programmer's point of view audio plugins are always compiled as dynamically loadable libraries that stricly conform to a format's specific API. The host application can load then at run time and stream audio data through them.

Additionally Realtime Audio Plugins, as the name implies, must be able to complete thier tasks fast enough to comply with realtime audio requirements. How fast is fast enough? Well, that depends how you define "real time". In audio applications, real time is defined in terms of an audio system's latency. The total delay between the time an audio signal enters the system (at the analog to digital converter for example), is processed, and leaves the system (at the digital to analog conterver) is the latency. If a musician is performing live and simultaneously hearing the result of the performance after being processed digitally, the system's latency must be low enough to feel instantaneous. The maximum acceptable latency is considered to be around 10ms\cite{AES67-2013}. Any higher and the latency becomes disturbing and not acceptable for live performance applications.

Any process will introduce some ammount of delay. Some processes, like those that rely on an FFT for example, need to work on a group of samples, introducing additional latency. Within the audio processing function, the programmer must take care not to introduce any unneccessary or uncalculateable delays. Examples for things to avoid are memory allocations, conditional expressions inside loops that might break pipe-lining optimizations\cite{realtime-architectures}, or updating the graphic interface directly.

\section{Audio Over Ethernet}

Sending audio data over a network is not new. Sending audio data in "realtime" is also not new. The IETF ( Internet Engineering Task Force ) RFC 3550 Describes the Real-time Transport Protocol for delivering audio and video in real-time over IP networks. RTP however is mostly concerned with transmitting a few channels of media as quickly as possible over IP networks with lot of other traffic, reducing jitter (variations in latency) and providing Quality of Service strategies to achieve "acceptable" audio and video quality for streaming and conferencing purposes.

Other specifications such as AVB\footnote{Audio Video Bridging refers to a set of IEEE standards that allow time-synchronized low latency streaming services } and AES67\footnote{AES67, created by the Audio Engineering Society, defines standard that allow existing low latency streaming systems to interoperate. AES67 does not define any new technologies but attempts to set a lowest common denominator by which existing standards can be compatible. } build on top of RTP and add more mechanisms to guarentee acurate timing and synchronization across a network for professional audio applications. The synchronisation is important is these applications because they are concerned with driving audio hardware attached to different hosts on a network.

Hardware synchronization and jitter management are not relevent to this project because we are not concerned with external audio hardware. The goal of this project it to utilise external cpus as audio coprocessors. Even so, the AVB and AES67 standards offer many insights into how to optimize data transmissionfor low latency applications and they also offer a proof-of-concept that it must be possible. The AES67 Specification's recomended packet times offer latencies well below 1 ms for hundred of simultaneos channels of high quality audio. This is weell above legacy PCI and Firewire rates used for DSP based coprocessing\cite{bouillot2009aes}.



\section{Single Board Computers}

\section{Virtual Analog Synthesis}