\section{Background}

An audio engineer's typical job is to manage the balance of multiple tracks of audio signals. The dynamic range of a signal can be compressed, in order to give quiter passages more presence. Loud peaks can be limited to balance the overall loudness of a musical piece. Using equalizers an audio engineer can make enhance or supress specific frequencies of a track to make it more present in a mix. Effects like reverb, echo, or chorus can be used to give a track more space in a mix effecting the mood or ambience of a music piece. It is typcial that each track in a recording session will be processed by a chain of several specific audio processors.

20 years ago the equipment responsible for these kinds of processes filled large racks. Today all of these tasks run as plugins on the CPU.

In 1996 Steinberg GmbH, the developers of Cubase, a popular audio production application released the VST interface specification and SDK.\cite{VST-wikipedia} The VST plugin standard was special because it allowed realtime processing of audio in the CPU and it allowed other developers to programm plugins which could be run from within Cubase. The VST plugin standard quickly had widespread industry acceptance and was adopted by most developers of audio production applications. Although alternative standards exist on Apple and Linux Operating Systems, VST is still the most widely adopted crossplatform standard.

The number of realtime plugins that could run on a CPU was limited by several factors, hard disc access speeds, bus speeds, amount of ram, OS schedulers for instance\cite{latency98}. User's didn't expect to be able to run more that 10 plugin instances at a time. Simply playing back multiple tracks of digital audio in realtime was so taxinig on the CPU that an application's graphical interface would quickly become unresponsive.

Today it's possible to playback hundreds of channels of audio and hundreds of plugins in realtime. But as the performance threshhold has risen, so to have the expectations. The algorithms driving today's plugins are much more complex that those from 1996. Plugins are available today that model physical systems or emulate the analog circutry of popular vintage synthsizers. So, even though CPU performance has risen significantly, it's still easy to reach the limits, especially with the more complex high qualtiy plugins.

Serval DSP based systems exist that can alleviate the load on the CPU much in the same way that GPU accelerator cards work. Audio processing jobs are delegated to external specialized hardware via PCIe or Thunderbolt interfaces. However, these DSP based accelerators are proprietary and expensive. Developing plugins for a DSP chip is also significatly more complex than developing for the CPU.

\section{Realtime Audio Plugins}

Music composition and production is typically done with the assistance of a music sequencing application. Midi events
 and audio recordings are arranged as tracks that can be mixed, edited, and processed. In order to make changes
 undo-able edits are made in a non-destructive fasion, calculated dynamically in realtime during audio playback. The
 original audio data is always preserved. The user can change the parameters of an effect or process in realtime and
 experiment with various parameters without fearing that the original audio recording might be permenantly altered.

A music sequencer or audio production application will usually include several built in realtime effects that a user
can apply to an audio track. In addition to the built in options all professional applications will also be able to
load 3d party effect plugins. Depending on the platform and vendor one or several available plugin standard will be
implmeneted, the most common standard being Steinberg's VST standard.

Regardless of the standard most audio plugins function in a similar fashion. The host application will periodically
poll the plugin via a callback, providing access to the source audio data stream and expecting the plugin to return the
 processed data. The size of the data provided at each callback is determined by the host application. By increasing the
 buffer size sent during each callback the CPU has more time to process the data, but also increases the latency of
 the signal.

 Audio plugins can also provide a gui to the user that allows processing parameters to be modified, saved, and
 sequenced as well. This might be the cutoff frequency of a low pass filter, or the delay time of a reverb effect,
 for example.

 On the Windows platform plugins are compiled as muti-threaded Dynamic Link Libraries, on Mac OSX they are Mach-O
 Bundles.

\section{Audio Over Ethernet}

\section{Single Board Computers}