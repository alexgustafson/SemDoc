\section{Echtzeit-Audio-Plug-ins}

Das Komponieren und Produzieren von Musik erfolgt in der Regel mit Hilfe einer Digital Audio Workstation(DAW-)-Software. MIDI-Events und Audio-Aufnahmen werden als Spuren arrangiert, gemischt, redigiert und verarbeitet. Um Änderungen rückgängig zu machen, werden Bearbeitungen in einem zerstörungsfreien Verfahren gemacht, das dynamisch in Echtzeit während der Wiedergabe berechnet wird. Die ursprünglichen Audiodaten bleiben immer im ursprünglichen Zustand erhalten. Der Benutzer kann die Parameter eines Effektes oder Prozesses in Echtzeit ändern und mit verschiedenen Einstellungen experimentieren, ohne zu fürchten, dass die ursprünglichen Audioaufzeichnungen geändert werden.

Eine DAW-Anwendung hat in der Regel mehrere Echtzeit-Effekte eingebaut, welche ein Benutzer auf Audiospuren einsetzen kann. Zusätzlich zu den eingebauten Effekten sind alle professionellen DAW-Anwendungen auch in der Lage, Dritthersteller-Plug-ins zu laden. In Abhängigkeit von der Plattform und dem Anbieter werden ein oder mehrere Plug-In-Standards implementiert. Steinbergs VST-Standard ist der meistverbreitete.

Alle Standards funktionieren in einer ähnlichen Weise. Die Host-DAW-Anwendung übergibt in regelmäßigen Abständen das Plug-in über eine Rückruffunktion an zu verarbeitende Audiodaten. Das Plug-in muss innerhalb eines definierten Zeitrahmens die Verarbeitung abgeschlossen haben und wartet dann auf den nächsten Abruf.

Audio-Plug-ins können auch eine grafische Oberfläche zur Verfügung stellen, über die ein Benutzerparameter geändert und gespeichert werden kann. Dies könnte z. B. die Grenzfrequenz eines Tiefpassfilters oder die Verzögerungszeit eines Hall-Effekts sein.

Aus der Sicht des Programmierers sind Plug-ins dynamisch ladbare Bibliotheken, die eine spezifizierte API implementieren. Die Host-DAW-Anwendung kann dann  während der Laufzeit laden und Audiodaten durch sie verarbeiten lassen\cite{realtime-architectures}. Auf der Windows-Plattform werden VST-Plug-ins als Dynamic Link Libraries (DLL) kompiliert, auf Mac OSX sind sie Mach-O-Bundles. Die  ursprünglichen Apple-AudioUnit-Plug-ins sind auch als Mach-O-Bundles zusammengestellt, sie haben fast identische Funktionalität. Andere alternative Plug-in-Formate sind Avids RTAS und AAX-Plug-Formate, Microsofts DirectX-Architektur oder LADSPA, DSSI und GW2 auf Linux.

Echtzeit-Audio-Plug-ins, wie der Name schon sagt, müssen in der Lage sein, ihre Aufgaben schnell genug zu erfüllen, um den Echtzeit-Audio-Anforderungen gerecht zu werden. Wie schnell ist schnell genug? Nun, das hängt davon ab, wie man ”Echtzeit” definiert. Für Audio-Anwendungen wird Echtzeit in Bezug auf die Latenz des Audiosystems definiert. Die Gesamtverzögerung zwischen dem Eintreffen eines Audiosignals in das System (beim Analog-Digital-Wandler zum Beispiel), Verarbeitung, und dem Verlassen des Systems (beim Digital-Analog-Wandler oder Audio-Ausgang) ist die Latenz. Die maximal zulässige Latenzzeit für Audioanwendung ist ungefähr 10 ms\cite{AES67-2013}. Ist die Latenzzeit länger als 10ms, wird sie  als störend empfunden und ist nicht mehr akzeptabel für Live-Performance-Anwendungen.

Jeder Verarbeitungsprozess wird eine gewisse Verzögerung hinzufügen. Doch innerhalb der Audio-Verarbeitungsfunktion muss der Programmierer darauf achten, keine unnötigen oder nicht berechenbaren Verzögerungen zu erzeugen. Es ist auch wichtig, zu verstehen, dass die Audio-Verarbeitungsfunktion im Kontext eines hoch-prioritären System-Threads berechnet wird. Nichts innerhalb dieser Funktion sollte auf Ressourcen warten, welche von anderen Threads mit niedrigerer Priorität berechnet werden. Beispiele von zu vermeidenden Dingen sind Speicherallokationen und Speicherdeallokationen oder das Warten auf bzw. Sperren eines Mutex\cite{realtime-architectures}, oder direkt Aktualisierungen der grafischen Oberfläche vorzunehmen.