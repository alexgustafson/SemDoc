\section{Realtime Audio Plugins}

Music composition and production is typically done with the assistance of a digital audio workstation (DAW) application. Midi events and audio recordings are arranged as tracks that can be mixed, edited, and processed. In order to make changes undo-able edits are made in a non-destructive fasion, calculated dynamically in realtime during audio playback. The original audio data is always preserved. The user can change the parameters of an effect or process in realtime and experiment with various parameters without fearing that the original audio recording might be permenantly altered.

A music sequencer or audio production application will usually include several built in realtime effects that a user can apply to an audio track. In addition to the built in options all professional applications will also be able to load 3d party effect plugins. Depending on the platform and vendor one or several available plugin standard will be implmeneted, the most common standard being Steinberg's VST standard.

Regardless of the standard most audio plugins function in a similar fashion. The host application will periodically poll the plugin via a callback, providing access to the source audio data stream and expecting the plugin to return the processed data.

Audio plugins can also provide a gui to the user that allows processing parameters to be modified, saved, and sequenced as well. This might be the cutoff frequency of a low pass filter, or the delay time of a reverb effect, for example.

On the Windows platform VST plugins are compiled to Dynamic Link Libraries, on Mac OSX they are Mach-O Bundles. The native apple Audio Unit plugins are also compiled as Mach-O bundles, they have almost identical functionality, but differ in their API implementation. Other alternative plugin formats are Avid's RTAS and AAX plugin formats, Microsoft's DirectX architecture, or LADSPA, DSSI and LV2 on Linux. From a programmer's point of view audio plugins are always compiled as dynamically loadable libraries that stricly conform to a format's specific API. The host application can load then at run time and stream audio data through them\cite{realtime-architectures}.

Additionally Realtime Audio Plugins, as the name implies, must be able to complete thier tasks fast enough to comply with realtime audio requirements. How fast is fast enough? Well, that depends how you define "real time". In audio applications, real time is defined in terms of an audio system's latency. The total delay between the time an audio signal enters the system (at the analog to digital converter for example), is processed, and leaves the system (at the digital to analog conterver) is the latency. If a musician is performing live and simultaneously hearing the result of the performance after being processed digitally, the system's latency must be low enough to feel instantaneous. The maximum acceptable latency is considered to be around 10ms\cite{AES67-2013}. Any higher and the latency becomes disturbing and not acceptable for live performance applications.

Any process will introduce some ammount of delay. Some processes, like those that rely on an FFT for example, need to work on a group of samples, introducing additional latency. Within the audio processing function, the programmer must take care not to introduce any unneccessary or uncalculateable delays. It's also important to understand that the audio processing function, called via a callback, is working the in context of a high-proirity system thread. Nothing in the process should have to lock or wait for resources calculated in other lower-priority threads. Examples of things to avoid are memory allocations or deallocation, waiting or locking a mutex conditional expressions inside loops that might break pipe-lining optimizations\cite{realtime-architectures}, or updating the graphic interface directly.

