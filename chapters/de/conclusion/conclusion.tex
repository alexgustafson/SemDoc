\section{Performance Evaluation}

An Audio Plugin runs is an environment with many other components, sharing CPU resources. Processing of audio data must be completed withing descreet time intervals dictated by the audio hardware and some setting the user can configure. Typical audio sampling frequencies are 44.1kHz, 48kHz, 88.2kHz and 96kHz. Using 44.1kHz as an example, this means that all the calculations required for a single sample must be completed within 0,023 ms. Interrupts would be recieved from the audio hardware at intervals of 0.023ms as well, and this would put too much of a strain on the Operating System of the CPU though. Instead, requests for new audio data are bundled into buffers of samples. The size of the buffers is a parameter that the user can modify, it's usually set at 512 samples, but can be as low as 16 samples.

Increasing the buffer size, increases the amount of time that the CPU has to provide the audio data to the audio hardware, this introduces latency into the system though. A buffer size of 512 samples equates to a latency of 11.60ms. A 16 sample buffer size equates to 0.36ms.

If a single plugin requires 1.0ms to complete its processing then it will not finish in time if the buffer size is set too low. If the buffer size is just high enough, then there still might not be enough CPU resources left for other plugins to complete their tasks.

\subsection{Synchronous Performance}

This project offloads the processing to an external SBC device. However, no resources are saved if the audio plugin is blocked while it waits for the results from the SBC device. Since the external SBC devices are slower than the main CPU, the processing time will take longer. Adding the time it takes to serialise and deserialise the Datagram packet at each end will degrade performance even more.

Figure~\ref{fig:local_vs_remote} illustrates the problem in more detail.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{assets/conclusion/process_flow_compared.pdf}
    \caption{Local vs Remote Synchronous Processing}
    \label{fig:local_vs_remote}
\end{figure}

The example shows the audio hardware polling the operating system in intervals of 5.8 ms. This corresponds to a buffer size of 256 samples. The time interval between states D and E represents the time it takes for a plugin to process a buffer of 256 sample. The DAW can perform other audio processing functions ( including passing the buffer to other plugins ) in the interval between F and G. After states G and H the DAW and OS are free to be able to do other things, like update the GUI or check for email.

With synchronous remote processing the time between states 1 and 2 in the Remote Process Flow will add directly to the time between D and E. If that time is significantly higher than the local processing then the ability of the DAW and the OS to perform other critical tasks is impaired.

\input{chapters/de/conclusion/sync_results}

Table~\ref{tab:latency_comp} shows the measured times for various buffer sizes in a remote synchronous environment that does no actually audio processing. An audio system running with a buffer size of 64 samples has 1.451ms to complete all tasks and provide the audio hardware with the resulting buffer. "rtTime" is the total round trip time. This corresponds to the time between states D and E of the Synchronous Remote Process Flow diagram in Figure~\ref{fig:local_vs_remote}. "pTime" is time interval between states 1 and 2 on the SBC device, the time spent processing the data. In this case there was no processing so this is only the measures the time spent deserialiseing and serialising the datagrams into audio and midi data. "tTime" is the "rtTime" subtracted by the "pTime" and is the packaging overhead required to send and receive data from the SBC device.

The last column, "\% of limit", shows how much time was spent in total as a percentage of the limit. At a buffer size of 64 samples 39.59\% percent of the total time available was spent by a single plugin. This does not leave much time for other processes on the CPU. For a buffer size of 512 samples the "\% of limit" value is much lower. The total system latency is slightly higher that the stated 10ms maximum though, and the synchronous design has not reduced the processing load on the CPU at all.


\subsection{Asynchronous Performance}

Instead of waiting for a response from the SBC device as in the synchronous method described above the asyncronous method checks if a response is available, if so process it, if not immediately return. The initial request for audio data would return empty, but by the time the second buffer is requested, the first respose from the SBC device might have returned. In both cases the only time spent by the audio plugin itself is the time is takes to serialise and send the data, and to receive and deserialise the data. These are the intervals between the states D and E and M and N represented in figure~\ref{fig:async_remote}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{assets/conclusion/async_flow.pdf}
    \caption{Local vs Remote Synchronous Processing}
    \label{fig:async_remote}
\end{figure}

By the time the second cycle begins (J in figure~\ref{fig:async_remote}) processed data from the first cycle has returned. The time between events M and N is no longer proportional to the amount of time needed to actuall process the data, only to the time it takes to prepare the data for sending and receiving. Table~\ref{tab:latency_async} shows the measured times.

\input{chapters/de/conclusion/async_results}

In the table above "rtTime" is no longer the actual round trip total processing time, but the "fake" one. Since data is already available at the time the interrupt request is triggered, the plugin can return it's data immediately. This comes at the cost of a delay equal to one interrupt cycle, but the user can set the audio system's buffer size low enough that this is well below the 10ms limit, without needing to use more than 1.2\% of the processing time available.

The asynchronous method brings a real benefit to distributed processing in terms of lighting the load on the CPU. The price of the benefit though is an increase in the latency of the plugin. The processed audio data is always one delayed by one buffer. Another disadvantage is that if several distributed processors are chained in series, as is the case in the demo appliation created for this project, then the latency is cumulative. This results in a total latency that is the limit time multiplied by the number of processors in the plugin.

\subsection{Optimisation Potential}

Several aspects of the application could be optimised to achieve higher performance and lower the latency times.

\subsubsection{Redundancy Reduction}

There are two areas of the application where unnecessary operations are performed and could be optimized. The first is the deserializsation of data when it is returned from the SBC. If serveral networked processes are chained in series, then the returned DiauproMessage could be sent directly to the following process without unnecessarily deserialising and the reserialising the data in between steps. This would be a relatively simple optimization to implement.

The second optimisation is similar but more complicated to implement. In a chain to processes the data would not need to be returned to the master plugin between each step, instead it could be sent to the next node directly. If the next node is located on the same SBC device then an additional hop over ethernet could be skipped as well. In addition serialisation steps between nodes could also be skipped if the nodes performed the operations directly on the data in the DiauproMessage. For this the DiauproMessage would need to be extended to include routing information so that each node knows the next destination point.

Although more complex, the second optimization would have the added benefit that the latency would no longer be a multiple of the full cycle time between interrupt calls. The final data could be gathered after a single cycle.


