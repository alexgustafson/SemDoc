\section{Performance Evaluation}

Eine Audio-Plugin läuft in einer Umgebung mit vielen anderen Komponenten mit dem es CPU-Ressourcen teilt. Die Verarbeitung von Audiodaten muss innerhalb diskreten, von der Audio-Hardware gesteuerten, Zeitintervallen abgeschlossen werden. Typische Audio-Sampling-Frequenzen sind 44,1 kHz, 48 kHz, 88,2 kHz und 96 kHz. Mit 44,1 kHz als Beispiel bedeutet dies, daß die für eine einzelne Abtastung erforderlichen Berechnungen innerhalb von 0,023 ms durchgeführt werden muss. Interrupts müssten von der Audio-Hardware in Abständen von 0.023ms getätigt werden, und dies würde das Betriebssystem der CPU überlasten. Stattdessen werden Anfragen für neue Audiodaten in Puffern gebündelt. Die Größe der Puffer ist ein Parameter, der vom Benutzer gestzt werden kann. In der Regel beträgt es 512 Abtastungen, kann aber bis 16 Abtastungen klein sein.

Erhöht man der Puffergröße steigt auch die Zeit, die die CPU hat, um die Audiodaten zu verarbeiten. Dies führt aber auch dazu das  die Latenz das System steigt. Eine Puffergröße von 512 Abtastungen entspricht einer Latenzzeit von 11.60ms. Ein 16-Probenpuffer Größe entspricht 0.36ms.

Braucht ein einzelnes Plugin 1,0 ms auf seine Verarbeitung abzushliessen, dann wird es nicht rechtzeigtig beendet, wenn die Puffergröße zu niedrig eingestellt ist. Wenn die Puffergröße gerade hoch genug ist, dann bleibe nicht genügend CPU-Ressourcen übrig für andere Plugins, ihre Berechnungen zu auszuführen.

\subsection{Synchronous Performance}

Dieses Projekt verteilt die Verarbeitung an externen SBC-Geräte. Es gibt jedoch keine Entlastung , wenn die Audio-Plugin blockiert ist, während es für die Ergebnisse aus dem SBC-Gerät wartet. Da die externe SBC-Geräte langsamer sind als der Haupt-CPU wird die Bearbeitungszeit sogar länger. Fügt man der Zeit hinzu, die zur Serialisierung und Deserialisierung das Datagramm-Paket an jedem Ende benötigt wird, wird die Leistung sogar noch verschlechtern.

Abbildung~\ref{fig:local_vs_remote} zeigt das Problem genauer.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{assets/conclusion/process_flow_compared.pdf}
    \caption{Local vs Remote Synchronous Processing}
    \label{fig:local_vs_remote}
\end{figure}

Das Beispiel zeigt die Audio-Hardware Abfrage des Betriebssystems, in Intervallen von 5,8 ms getacktet. Dies entspricht einer Puffergröße von 256 Abtastwerten. Das Zeitintervall zwischen den Staaten D und E repräsentiert die Zeit, die ein Plugin braucht, um einen Puffer von 256 Proben zu verarbeiten. Die DAW-Anwendung führt andere notwendige Audio-Verarbeitungsfunktionen in dem Intervall zwischen F und G. Nach Zustände G und H sind der DAW-Anweung und Betriebssystems wieder in der Lage andere Dinge zu tun, wie z.B. die GUI zu aktualisieren.

Mit der synchronen verteilte Verarbeitung wird der Zustand E blockiert bis Zuständen 1 und 2 abgeschlossen sind. Wenn diese Zeit signifikant ist dann hindert dies die DAW-Anwendung und Betriebssystem andere wichtige Aufgaben durchzuführen.

\input{chapters/de/conclusion/sync_results}

Table~\ref{tab:latency_comp} shows the measured times for various buffer sizes in a remote synchronous environment that does no actually audio processing. An audio system running with a buffer size of 64 samples has 1.451ms to complete all tasks and provide the audio hardware with the resulting buffer. "rtTime" is the total round trip time. This corresponds to the time between states D and E of the Synchronous Remote Process Flow diagram in Figure~\ref{fig:local_vs_remote}. "pTime" is time interval between states 1 and 2 on the SBC device, the time spent processing the data. In this case there was no processing so this is only the measures the time spent deserialiseing and serialising the datagrams into audio and midi data. "tTime" is the "rtTime" subtracted by the "pTime" and is the packaging overhead required to send and receive data from the SBC device.

The last column, "\% of limit", shows how much time was spent in total as a percentage of the limit. At a buffer size of 64 samples 39.59\% percent of the total time available was spent by a single plugin. This does not leave much time for other processes on the CPU. For a buffer size of 512 samples the "\% of limit" value is much lower. The total system latency is slightly higher that the stated 10ms maximum though, and the synchronous design has not reduced the processing load on the CPU at all.


\subsection{Asynchronous Performance}

Instead of waiting for a response from the SBC device as in the synchronous method described above the asyncronous method checks if a response is available, if so process it, if not immediately return. The initial request for audio data would return empty, but by the time the second buffer is requested, the first respose from the SBC device might have returned. In both cases the only time spent by the audio plugin itself is the time is takes to serialise and send the data, and to receive and deserialise the data. These are the intervals between the states D and E and M and N represented in figure~\ref{fig:async_remote}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{assets/conclusion/async_flow.pdf}
    \caption{Local vs Remote Synchronous Processing}
    \label{fig:async_remote}
\end{figure}

By the time the second cycle begins (J in figure~\ref{fig:async_remote}) processed data from the first cycle has returned. The time between events M and N is no longer proportional to the amount of time needed to actuall process the data, only to the time it takes to prepare the data for sending and receiving. Table~\ref{tab:latency_async} shows the measured times.

\input{chapters/de/conclusion/async_results}

In the table above "rtTime" is no longer the actual round trip total processing time, but the "fake" one. Since data is already available at the time the interrupt request is triggered, the plugin can return it's data immediately. This comes at the cost of a delay equal to one interrupt cycle, but the user can set the audio system's buffer size low enough that this is well below the 10ms limit, without needing to use more than 1.2\% of the processing time available.

The asynchronous method brings a real benefit to distributed processing in terms of lighting the load on the CPU. The price of the benefit though is an increase in the latency of the plugin. The processed audio data is always one delayed by one buffer. Another disadvantage is that if several distributed processors are chained in series, as is the case in the demo appliation created for this project, then the latency is cumulative. This results in a total latency that is the limit time multiplied by the number of processors in the plugin.

\subsection{Optimisation Potential}

Several aspects of the application could be optimised to achieve higher performance and lower the latency times.

\subsubsection{Redundancy Reduction}

There are two areas of the application where unnecessary operations are performed and could be optimized. The first is the deserializsation of data when it is returned from the SBC. If serveral networked processes are chained in series, then the returned DiauproMessage could be sent directly to the following process without unnecessarily deserialising and the reserialising the data in between steps. This would be a relatively simple optimization to implement.

The second optimisation is similar but more complicated to implement. In a chain to processes the data would not need to be returned to the master plugin between each step, instead it could be sent to the next node directly. If the next node is located on the same SBC device then an additional hop over ethernet could be skipped as well. In addition serialisation steps between nodes could also be skipped if the nodes performed the operations directly on the data in the DiauproMessage. For this the DiauproMessage would need to be extended to include routing information so that each node knows the next destination point.

Although more complex, the second optimization would have the added benefit that the latency would no longer be a multiple of the full cycle time between interrupt calls. The final data could be gathered after a single cycle.


